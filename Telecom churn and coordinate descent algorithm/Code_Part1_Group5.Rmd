---
title: "Code_Part1"
author: "Group 5"
date: "2023-12-08"
output: pdf_document
---

\section{Real World Data} 
A well-known business problem of companies in every industry is related to the "customer churn". This phenomenon happens if customer of a certain firm decides to stop doing business with it. Usually, for companies is less costly to retain a customer with respect to obtain a new one.

The dataset that we decided to analyse contains information on 3150 customers of an Iranian telecommunication company aggregated over a time period of 9 months. The variable we are interested in predicting is Churn, which is binary and it assumes the value 0 if the company successfully retained the respective customer after 12 months from when this study began, and the value 1 if the customer left the company. We are interested in studying the possibility of whether a customer will "churn" or not. Being able to correctly predict this fact could lead to smaller customers loss for firms, and, hence, to major profits and customer loyalty.

Below, we explore the variables present in the data set. 
First, we notice that there are not missing values and that there is a class imbalance in favour of the 0 class (not churn). Second, from the names of the columns (\textit{Call Failure, Complains, Subscription, Length, Charge Amount, Seconds of Use, Frequency of use, Frequency of SMS, Distinct Called Numbers, Age Group, Tariff Plan, Status, Age, Customer Value, Churn}) we notice that two of them are related to the age of customers: Age and Age.Group, with the latter being a transformation of Age that we decided to not use. From the set of 12 predictors that we are left with, we are interested in the ones which have the largest impact on the probability of a customer to churn. We expect that these variables might be Complains and Status. A high number of complaints for a customer usually translate to a bad experience with the services, therefore, we expect that a positive correlation with the probability to churn. Furthermore, the status of the customer, being active or non-active, might reflect the familiarity that a customer has with the service: we expect that a non-active customer is less likely to switch to a different company with respect to someone who frequently uses the service and is more aware of the quality of the service that they are receiving.

```{r echo=T, results='hide',message=FALSE}
lapply(c('ggplot2','plotly','class','lattice','caret','pROC','MASS','QuantPsyc'),
       require,character.only=T)
df=read.csv("customer_churn.csv")
sum(is.na(df)) # number of missing values is zero
sum(df$Churn==0)/nrow(df) # percentage of observation that did not churn: 84%
df<-subset(df, select = -Age.Group) # deleting age group
```
\subsection{Data Visualisation}
The plot is obtained after applying PCA on our data. In particular, the first 3 components, which explain over 60% of the variance, were extracted. This dimensionality reduction allow us to visualise and understand the structure of the data. As we can see from the 3D plot, the decision boundary between the 2 classes of churn looks to be moderately non-linear. Hence, we expect that QDA might be the best classification approach for this data.
```{r} 
churn <- df$Churn
data_without_churn <- subset(df, select = -Churn) # data set without the churn column
data_scaled <- scale(data_without_churn) # standardizing
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(pca_result$x)
# 3D Plotting
pca_data$Churn <- churn
fig <- plot_ly(data = pca_data, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Churn,
               colors = colorRampPalette(c("blue", "red"))(100), 
               type = "scatter3d", mode = "markers")
fig <- fig %>% layout(title = "3D PCA Plot Colored by Churn", 
                      scene = list(xaxis = list(title = "PC1"), 
                      yaxis = list(title = "PC2"), zaxis = list(title = "PC3")))
```
\subsection{Methods}
Given that our goal is to evaluate the performance of different classification methods on our dataset, the approaches we will study are the following: KNN, LDA/QDA and Logistic Regression. 

\subsubsection{kNN}
The tuning of the hyperparameter $k$ is done by performing a 5-fold cross-validation and the aim is to find the value of $k$ that minimizes the validation error.
The metrics considered to evaluate the performance are the values of AUC and MER.
```{r results='hide',message=FALSE}
set.seed(301)
train_index = sample(1:nrow(df),nrow(df)*80/100)
# This is to split the data 80/20 for training and testing respectively.
train = df[train_index,]       
test = df[-train_index,]      
folds <- createFolds(train$Churn, k = 5, list = TRUE, returnTrain = TRUE)
# In the line above we create 5 fold of data, which will each be used once to train the data.
k = seq(1,100,1)  # vector of all the possible k values   
MVE_of_k_equals_elem=c() # the mean validation error of each k            
for (elem in k){
  valid_errors_k_equals_elem = c()
  for (i in seq_along(folds)) {
    train_indices <- folds[[i]]
    train_data <- train[train_indices, ]
    # In line above the train data is specified
    valid_data <- train[-train_indices, ]
    # In the line above we specify the validation data
    pred = knn(train_data, valid_data, k=elem,cl=train_data$Churn)
    # In the line above we use the knn testing against the validation data
    MER = mean(pred!=valid_data$Churn)
    # In the code below we output the mean error rate produced from our validation step
    valid_errors_k_equals_elem=c(valid_errors_k_equals_elem,MER)
  }
  MVE_of_k_equals_elem=c(MVE_of_k_equals_elem, mean(valid_errors_k_equals_elem))
}
k_min = which.min(MVE_of_k_equals_elem)  # Value of k that minimises the MER
y_pred = as.numeric(as.character(knn(train,test,k=k_min, cl=train$Churn)))
# The predicted classes using the optimal k
conf_table = table(test$Churn,y_pred)  
MER_KNN = (conf_table[2]+conf_table[3])/dim(test)[1]
MER_KNN # misclassification error rate for the knn
prob_knn = as.numeric(as.character(knn(train, test, cl = train$Churn, k = k_min, 
                                       prob = TRUE)))
# return the probability of success needed as argument in the roc function
perf_KNN = roc(response=test$Churn,predictor=prob_knn,legacy.axes=T)
AUC_KNN = as.numeric(perf_KNN$auc) 
AUC_KNN # performance of knn
test_knn<-cbind(test, y_pred)
churn_prediction_correct_knn<-test_knn$y_pred*test_knn$Churn
```

\subsubsection{Logistic regression}
The key hyperparameter in logistic regression is the number of variables selected. The tuning is done by performing a forward step-wise selection, starting from the null model and subsequently adding variables based on AIC minimization. The forward step-wise algorithm finds a local optimum model, whereas the best subset selection method finds the global optimum. However, the latter would be much more computationally intensive, since it would need to fit and evaluate $2^{p}=2^{12}$= 4096 models.
```{r results='hide',message=FALSE}
set.seed(300)
min.model <- glm(Churn ~ 1, family=binomial(link = "logit"), data=train)
max.model <- glm(Churn ~ ., family=binomial(link = "logit"), data=train)
glm.step <- step(min.model, scope = list(lower = min.model, upper = max.model),
              direction="forward", trace = 0)
summary(glm.step)
# The code above finds performs the forward step-wise algorithm on the logistic regression.
test1 = data.frame(test)
y_pred = predict(glm.step, newdata = test1, type = "response")
y_pred[y_pred<0.5] = 0 # initial threshold of 0.5
y_pred[y_pred>=0.5 & y_pred!=0] = 1
table1 = table(test$Churn,y_pred)  # Comparison between true and predicted values
MER_LOG = (table1[2]+table1[3])/dim(test)[1]
MER_LOG # misclassification error rate for the logistic regression
y_pred = predict(glm.step, newdata = test1, type = "response")
perf_LOG = roc(response=as.numeric(test$Churn),predictor=as.numeric(y_pred))
AUC_LOG = as.numeric(perf_LOG$auc)
AUC_LOG # performance of the logistic regression
test_glm<-cbind(test, y_pred)
churn_prediction_correct_glm<-test_glm$y_pred*test_glm$Churn
```
The analysis of the logistic regression output highlights that the predictor which has the most impact on the response variable is Complains, which meets our initial expectations.  
Moreover, the results support our intuition that churning is most likely to take place among active users than inactive ones.

\subsubsection{LDA and QDA}
In this section we omit any discrete variables from our model, since discriminant analysis assumes that the data, given a class, follows the Multivariate Normal (MVN) distribution.

```{r results='hide',message=FALSE}
set.seed(302)
lda1 <- lda(Churn ~ Call..Failure + Subscription..Length + Seconds.of.Use +
              Frequency.of.use + Frequency.of.SMS + Distinct.Called.Numbers +
              Age + Customer.Value, data = train)
test1 = data.frame(test)
y_pred1 = as.numeric(as.character(predict(lda1, newdata = test1)$class))
# to get the classes of the prediction from a list we do $class
table1 <- table(test$Churn,y_pred1)
MER_LDA = (table1[2]+table1[3])/dim(test)[1]
MER_LDA # misclassification error rate for LDA
perf_LDA = roc(response=as.numeric(test$Churn),
               predictor=predict(lda1, newdata = test1)$posterior[,2])
# the probability of success is found in the 2nd column of the $posterior matrix
AUC_LDA = as.numeric(perf_LDA$auc) 
test_lda <- cbind(test, y_pred1)
churn_prediction_correct_lda <- test_lda$y_pred1*test_lda$Churn

lda2 <- qda(Churn ~ Call..Failure + Subscription..Length + Seconds.of.Use +
              Frequency.of.use + Frequency.of.SMS + Distinct.Called.Numbers +
              Age+Customer.Value, data = train)
test1 = data.frame(test)
y_pred2 = as.numeric(as.character(predict(lda2, newdata = test1)$class))
table2 <- table(test$Churn,y_pred2)
MER_QDA = (table2[2]+table2[3])/dim(test)[1]
MER_QDA # misclassification error rate for QDA
perf_QDA = roc(response=as.numeric(test$Churn),
               predictor=predict(lda2, newdata = test1)$posterior[,2])
AUC_QDA = as.numeric(perf_QDA$auc)   
test_qda <- cbind(test, y_pred2)
churn_prediction_correct_qda<-test_qda$y_pred2*test_qda$Churn
```

\subsection{Conclusion}
The table below shows that the best approach for the analysed dataset is Logistic Regression, with an AUC value over 10% higher than the second best alternative.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
& KNN & Logistic Regression & LDA & QDA \\
\hline
MER & $`r round(MER_KNN,2) `$ &  $`r round(MER_LOG,2) `$ & $`r round(MER_LDA,2)`$ & $`r round(MER_QDA,2)`$\\
\hline
AUC & $`r round(AUC_KNN,2) `$ &  $`r round(AUC_LOG,2) `$ & $`r round(AUC_LDA,2)`$ & $`r round(AUC_QDA,2)`$\\
\hline
Profit & $`r round(profit_knn,2) `$ & $`r round(profit_LOG,2) `$ & $`r round(profit_LDA,2) `$ & $`r round(profit_QDA,2) `$\\
\hline

\end{tabular}
\end{center}
\end{table}
However, our initial expectation about QDA outperforming did not occur. A reasonable explanation might be that, while moderately non-linear data is an indicator of the suitability of QDA, discriminant analysis requires also the assumption that the data, given each class, is drawn from an MVN distribution. 
We can check the normality of data by performing a Mardia test, which has as null hypothesis the normality of data. As we can see from the output, the p-value is 0, which means that the data is not normal. 
```{r}
data_without_discrete<-subset(df[df$Churn==0,], select = -c(Complains, Charge..Amount, 
                                                          Tariff.Plan, Status, Churn))
mult.norm(data_without_discrete)$mult.test
```
We perform this test only on the observations where churn is 0, as normality should hold for all the observations in a given class. The combination of a lack of normality and linearity explains why, out of all the approaches used, LDA and QDA yielded the smallest AUC value by a large margin.
It follows a graphical representation of the ROCs of the analysed methods:
```{r}
plot(perf_KNN,col='red')
plot(perf_LOG,col='blue',add=T)
plot(perf_LDA,col='green',add=T)
plot(perf_QDA,col='purple',add=T)
legend("topright",legend = c('kNN','Logistic','LDA','QDA'), lty = c(1,1), 
       col = c('red','blue','green','purple'))
```
We can see how the blue line, corresponding to the logistic regression, covers the greatest area.

In conclusion, we would like to propose a metric to assess the performance of each approach in terms of profit and cost. The telecom company plans to use the prediction to intervene offering a special discount to a potentially churning customer. This policy will lead to a cost, which we assume to be the 40% of the value of the customer. For customers which would have ultimately churned, a correct prediction will result in positive returns, as it is more costly to acquire a new customer than replacing them. However, for customers who would not have churned, a wrong prediction would lead to an economic loss for the company. 
Hence, we can model the profit generated by the company using a method, called A, that works as follows: $\pi(A) = \sum_{i=1}^{N}[ v_i*I(y_i=\hat{y_i}=0)-0.4*v_i*I(\hat{y_i}=0)]$, where $N$ is the number of data points in the test set. Note that the formula suggests that the profits generated by the model are the sum of the values of all customers that we predict as staying and are actually doing that, minus a proportion of the values of who would have left, but did not, thanks to the company intervention that we assume to be always effective. Indeed, a lack of intervention by the company would result in all of them churning and, in turn, to the loss of the sum of their values. Nevertheless, this formula is not contemplating the economic loss of the customers churning, despite the prediction of them staying and in this case we are largely approximating the estimate based on the available data.

The results below show that the method which the company should use is the logistic regression, as it will maximise its profits.
Also, it is interesting to see how the QDA yields a strongly negative profit value. This is explainable by noticing that QDA has a particularly high value of FPR compared to the other methods. Hence, the company would end up in spending a lot of money to retain customers that would have stayed in any case. 


```{r results='hide',message=FALSE}
profit_knn = sum(as.numeric(churn_prediction_correct_knn)*test_knn$Customer.Value-
                   0.4*test_knn$Customer.Value*as.numeric(test_knn$y_pred))
profit_LOG = sum(churn_prediction_correct_glm*test_glm$Customer.Value-
                   0.4*test_glm$Customer.Value*test_glm$y_pred)
profit_LDA = sum(as.numeric(churn_prediction_correct_lda)*test_lda$Customer.Value-
                   0.4*test_lda$Customer.Value*as.numeric(test_lda$y_pred))
profit_QDA = sum(as.numeric(churn_prediction_correct_qda)*test_qda$Customer.Value-
                   0.4*test_qda$Customer.Value*as.numeric(test_qda$y_pred))
```

For future exploration, alternative methods such as Random Forests, Decision Trees, Bagging, Boosting and SVM could be considered to further enhance predictive accuracy, to explore the interplay of various factors in customer churn dynamics and to capture better the potential relationship between costumer churn and economic profit. In addition, we believe that the dataset we worked on does not include predictors which would be significant for customers of today. For instance, the speed at which clients access online contents could reflect their level of satisfaction towards the service provided. 

Finally, in the profit model we make the assumption that the company intervention on predicted-to-churn customers is always effective. This is a strong assumption, and, for instance, the effectiveness of the policy of the firm might be represented by a probability distribution.