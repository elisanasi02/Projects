---
title: "Code_Part2"
author: "Group 5"
date: "2023-12-08"
output: pdf_document
---

\section{Coordinate descent algorithms}
In this section we will apply coordinate descent algorithms on penalized regression problems, in particular on the LASSO and on the elastic net.
This algorithm permits to obtain solutions for convex optimization problems in a faster way: this is due to the "one-at-a-time" method, which updates the beta components as we iteratively examine each predictor (Friedman, 2007).
In the following section we show the code for the algorithm applied in the case of LASSO and elastic net.

\subsection{Implementation of the algorithms}
```{r}
# First algorithm: lasso coordinate descent
lasso_coordinate_descent <- function(X, y, lambda, max_iter, tolerance) {
  n = length(y)
  p = ncol(X)
  beta0 = rep(0 ,p)
  beta_old = beta0
 
  for (iter in 1:max_iter) {
    for (j in 1:p) {
      # Compute partial residuals
      rij = y - X %*% beta0 + X[, j] * beta0[j]
     
      # Compute simple least squares coefficient of these residuals
      beta_star = sum(X[, j] * rij) / (n)
     
      # Update beta_j by soft thresholding
      beta0[j] = sign(beta_star) * max(0, abs(beta_star) - lambda)
    }
    # Check for convergence
    if ((sum((beta0 - beta_old)^2)) <= tolerance) {
      break
    }
    else{
      if(sum((beta0 - beta_old)^2)>1000){
         return(rep(0,p))}
      }
    beta_old = beta0
 
  }
  return(beta0)
}
```

In the code, $n$ indicates the number of observation, $p$ is the number of predictors, $\lambda$ is the tuning parameter, while \textit{tolerance} is an arbitrary small value used to check the convergence of the $\boldsymbol{\beta}$. 


```{r}
# Second algorithm: elastic net coordinate descent
el_coordinate_descent <- function(X, y, lambda1, lambda2, max_iter, tolerance) {
  n = length(y)
  p = ncol(X)
  beta0 = rep(0 ,p)
  beta_old = beta0
 
  for (iter in 1:max_iter) {
    for (j in 1:p) {
      # Compute partial residuals
      rij = y - X %*% beta0 + X[, j] * beta0[j]
     
      # Compute simple least squares coefficient of these residuals
      beta_star = sum(X[, j] * rij) / (n)
     
      # Update beta_j by soft thresholding
      beta0[j] = sign(beta_star) * max(0, (abs(beta_star) - lambda2)) / (1 + 2 * lambda1)
    }
    # Check for convergence
    if ((sum((beta0 - beta_old)^2)) <= tolerance) {
      break
    }
    else{
      if(sum((beta0 - beta_old)^2)>1000){
         return(rep(0,p))}
      }
    beta_old = beta0
  }
  return(beta0)
}
```
The main and only difference between the two algorithms is located in the rule of soft thresholding.

While executing these algorithms, we notice that, in some cases, the beta coefficients had the tendency to inflate at each iteration. This resulted in the norm of the vector to reach considerably high values, to the point that \texttt{R} consider it to be \texttt{Inf}. By consequence, this led to the impossibility of evaluating the convergence and, to avoid this issue, we added a further check on the value of the norm: if this value becomes bigger than 1000, we set the beta components to be zeros.

Investigating the literature, we believe that our issue is related to Tseng (2001). The paper highlight that for non-(pseudo)convex function, \textit{"the method may cycle without approaching any stationary point of $f$"}. For this reason, we are led to think that in cases involving specific combinations of data, predictors and hyperparameters, the function that we aim to minimise becomes nonconvex. Therefore, the coordinate descent method starts to generate bigger values and reaches infinity.

\subsection{Evaluation of the performances}
To evaluate the performance of our algorithms we run several simulations and consider three main scenarios with simulated data.
In particular, data are generated from the model $\textbf{y} = \textbf{X}\beta+\sigma\epsilon$ where $\epsilon\sim N(0,\textbf{I}_n)$. For 50 times, we generate from a multivariate normal distribution a set of 20 independent training data, a set of 20 validation data and a set of 200 testing data.

The three considered scenarios are:
\begin{itemize}
  \item The pairwise correlation between $\textbf{X}_i$ and $\textbf{X}_j$ is set to be     $corr(i,j)=0.85^{|i-j|}$, which is the covariance matrix for the MVN distribution, where each predictor and the response are standardized. Moreover, the $\boldsymbol\beta$ is set to be $\boldsymbol\beta = (3,1.5,0,0,2,0,0,0)^T$ and $\sigma = 3$.
  \item 3 predictors are considered: 2 of them have a high correlation (0.9) and the third one is nearly uncorrelated with the others.
  \item $p$ progressively becomes higher than $n$. In this scenario a specific correlation structure has not been set and the beta values are randomly chosen, but they are never zero.
\end{itemize}

In general, when there is a considerable correlation structure between the predictors, we expect LASSO to select fewer variables than elastic net. And, particularly for the second scenario, where we have two strongly correlated predictors, we expect LASSO to select only one of the two.
In addition, under the scenarios we considered, we expect elastic net to predominantly outperform LASSO, especially in the third scenario ($p>n$). This is due to the fact that LASSO should saturate and choose $n$ predictors at most, even if more than $n$ coefficients are known to be non-zero.

In the following sections we provide the results for each scenarios.

\subsubsection{Scenario 1}
```{r}
outperf1 = 0 # amount of times elastic net MSE is worse than lasso
equalperf1 = 0 # amount of times elastic net MSE is equal to lasso
nonzero_el_list1 = c() # list of non zero coefficients for every simulation
nonzero_lasso_list1 = c() 
mse_el_list1 = c()
mse_lasso_list1 = c()

for (sim in 1:1){ # Setting 50 simulations
p = 8
r = matrix(NA,p,p)
for (i in 1:p){
  for (j in 1:p){
    r[i,j]= (0.85)^(abs(i-j))
  }
}
# The double for-loop structure generates r, the covariance matrix of the 
# MVN distribution between the predictors

# Generating X
n.train = 20
n.validation = 20
n.test = 200
n = n.train + n.validation + n.test
  
library(MASS)
X.train = mvrnorm(n=n.train, Sigma=r, mu=rep(0,p))
X.train = as.matrix(X.train)

X.validation = mvrnorm(n=n.validation, Sigma=r, mu=rep(0,p))
X.validation = as.matrix(X.validation)

X.test = mvrnorm(n=n.test, Sigma=r, mu=rep(0,p))
X.test = as.matrix(X.test)

X = rbind(X.train,X.validation,X.test)

# Setting beta, sigma and epsilon
B = c(3, 1.5, 0, 0, 2, 0, 0, 0)
sigma = 3
epsilon = mvrnorm(n=1, Sigma=diag(n), mu=rep(0,n))

# Generating Y
Y = X %*% B + sigma * epsilon
Y = as.vector(Y)
Y = scale(Y)

y.train = Y[1 : n.train]
y.validation = Y[(n.train+1) : (n.train+n.validation)]
y.test = Y[(n.train+n.validation+1) : n]

max_iter = 1000
tolerance = 1e-4

# LASSO hyperparameter tuning
lambda_list = seq(0,3,0.01)
mse_list = c()
for (lambda in lambda_list){
  lasso_beta <- lasso_coordinate_descent(X.train, y.train, lambda=lambda,
                                         max_iter, tolerance)
  y_hat = X.validation %*% lasso_beta
  mse_list= c(mse_list, mean((y_hat-y.validation)^2))
}
# Above we are computing the MSE of the LASSO regression on the validation set for 
# each value of lambda between 0 and 3, with an increase of 0.01 for each iteration

# Selecting the value of lambda that gives the lowest MSE
lambda_min_lasso = lambda_list[which.min(mse_list)]

# Once we obtained the value of lambda, we proceed to run the coordinate descent 
# algorithm and to evaluate its performance by measuring the MSE on the test data.

# Running the algorithm of the coordinate descent for LASSO
lasso_beta_min <- lasso_coordinate_descent(X.train, y.train, lambda=lambda_min_lasso,
                                           max_iter, tolerance)

# Computing the predicted values for y using the beta obtained from the algorithm
y_hat_min_lasso = X.test %*% lasso_beta_min

# Computing the MSE on the test
mse_test_lasso = mean((y_hat_min_lasso-y.test)^2)

nonzero_lasso = sum(lasso_beta_min != 0) # sum of the non-zero coefficients

# We proceed analogously with the elastic net coordinate descent algorithm
mse_list_el = c()
lambda1_list = seq(0,1,0.01)
n1 = length(lambda1_list)
lambda2_list = seq(0,1,0.01)
n2 = length(lambda2_list)
a = 1
matrix = matrix(nrow = n1*n2, ncol = 3)
for (i in 1:n1){
  for (j in 1:n2){
    el_beta <- el_coordinate_descent(X.train, y.train, lambda1=lambda1_list[i], 
                                     lambda2=lambda2_list[j], max_iter, tolerance)
    y_hat = X.validation %*% el_beta
    matrix[a,] = c(lambda1_list[i], lambda2_list[j], mean((y_hat-y.validation)^2))
    a = a + 1
   }
}

index = which.min(matrix[,3])
lambda_min_el = matrix[index,]

el_beta_min <- el_coordinate_descent(X.train, y.train, lambda1=lambda_min_el[1],
                                     lambda2=lambda_min_el[2], max_iter, tolerance)

y_hat_min_el = X.test %*% el_beta_min
mse_test_el = mean((y_hat_min_el-y.test)^2)

nonzero_el = sum(el_beta_min != 0) 
nonzero_el_list1 = c(nonzero_el_list1, nonzero_el)
nonzero_lasso_list1 = c(nonzero_lasso_list1, nonzero_lasso)
mse_el_list1 = c(mse_el_list1, mse_test_el)
mse_lasso_list1 = c(mse_lasso_list1, mse_test_lasso)

if (mse_test_lasso > mse_test_el){
  outperf1 = outperf1 + 1
} # Counting how many times elastic net outperforms LASSO according to the MSE
if(mse_test_lasso == mse_test_el){
  equalperf1 = equalperf1 + 1
} # Counting how many times the performance of the two methods are equal
}
```


\subsubsection{Scenario 2}
```{r}
outperf2 = 0 # amount of times elastic net MSE is worse than lasso
equalperf2 = 0 # amount of times elastic net MSE is equal to lasso
nonzero_el_list2 = c() # list of non zero coefficients for every simulation
nonzero_lasso_list2 = c() # list of non zero coefficients for every simulation
mse_el_list2 = c()
mse_lasso_list2 = c()

for (sim in 1:1){
p = 3
r = matrix(NA,p,p) # Setting the correlation structure for this scenario
for (i in 1:p){
  for (j in 1:p){
    if (i==j){
      r[i,j]=1
    }else{
      if ((i==1&j==2)|(i==2&j==1)){
      r[i,j]=0.9
    }else{
      r[i,j]=0.1
    }}}}

n.train = 20
n.validation = 20
n.test = 200
n = n.train + n.validation + n.test
  
X.train = mvrnorm(n=n.train, Sigma=r, mu=rep(0,p))
X.train = as.matrix(X.train)
X.validation = mvrnorm(n=n.validation, Sigma=r, mu=rep(0,p))
X.validation = as.matrix(X.validation)
X.test = mvrnorm(n=n.test, Sigma=r, mu=rep(0,p))
X.test = as.matrix(X.test)
X = rbind(X.train,X.validation,X.test)

B = c(3, 1.5, 2)
sigma = 3
epsilon = mvrnorm(n=1, Sigma=diag(n), mu=rep(0,n))

Y = X %*% B + sigma * epsilon
Y = as.vector(Y)
Y = scale(Y)

y.train = Y[1 : n.train]
y.validation = Y[(n.train+1) : (n.train+n.validation)]
y.test = Y[(n.train+n.validation+1) : n]

max_iter = 1000
tolerance = 1e-4
lambda_list = seq(0,1,0.01)
mse_list = c()
for (lambda in lambda_list){
  lasso_beta <- lasso_coordinate_descent(X.train, y.train, lambda=lambda,
                                         max_iter, tolerance)
  y_hat = X.validation %*% lasso_beta
  mse_list= c(mse_list, mean((y_hat-y.validation)^2))
}

lambda_min_lasso = lambda_list[which.min(mse_list)]
lasso_beta_min <- lasso_coordinate_descent(X.train, y.train, lambda=lambda_min_lasso,
                                           max_iter, tolerance)
y_hat_min_lasso = X.test %*% lasso_beta_min
mse_test_lasso = mean((y_hat_min_lasso-y.test)^2)
nonzero_lasso = sum(lasso_beta_min != 0)

mse_list_el = c()
lambda1_list = seq(0,1,0.01)
n1 = length(lambda1_list)
lambda2_list = seq(0,1,0.01)
n2 = length(lambda2_list)
a = 1
matrix = matrix(nrow = n1*n2, ncol = 3)
for (i in 1:n1){
  for (j in 1:n2){
    el_beta <- el_coordinate_descent(X.train, y.train, lambda1=lambda1_list[i],
                                     lambda2=lambda2_list[j], max_iter, tolerance)
    y_hat = X.validation %*% el_beta
    matrix[a,] = c(lambda1_list[i], lambda2_list[j], mean((y_hat-y.validation)^2))
    a = a + 1
   }
}

index = which.min(matrix[,3])
lambda_min_el = matrix[index,]

el_beta_min <- el_coordinate_descent(X.train, y.train, lambda1=lambda_min_el[1], 
                                     lambda2=lambda_min_el[2],max_iter,tolerance)
y_hat_min_el = X.test %*% el_beta_min
mse_test_el = mean((y_hat_min_el-y.test)^2)

nonzero_el = sum(el_beta_min != 0)
nonzero_el_list2 = c(nonzero_el_list2, nonzero_el)
nonzero_lasso_list2 = c(nonzero_lasso_list2, nonzero_lasso)
mse_el_list2 = c(mse_el_list2, mse_test_el)
mse_lasso_list2 = c(mse_lasso_list2, mse_test_lasso)

if (mse_test_lasso > mse_test_el){
  outperf2 = outperf2+ 1
}
if(mse_test_lasso == mse_test_el){
  equalperf2 = equalperf2 + 1
}
}
```

The table below shows the results for a first comparison for scenario 1 and 2.
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
& Scenario 1 & Scenario 2 \\
\hline
Lasso MSE is higher (\%) & $`r round(outperf1 / 30 * 100,2) `$ &  $`r round(outperf2 / 30 * 100,2) `$\\
\hline
Lasso MSE is lower (\%) & $`r round((30-outperf1-equalperf1) / 30 * 100,2) `$ & $`r round((30-outperf2-equalperf2) / 30 * 100,2) `$\\
\hline
MSEs are equal & $`r round(equalperf1 / 30 * 100,2) `$ &  $`r round(equalperf2 / 30 * 100,2) `$\\
\hline
Lasso non-zero coeff & $`r round(mean(nonzero_lasso_list1),2) `$ & $`r round(mean(nonzero_lasso_list2),2) `$\\
\hline
EN non-zero coeff & $`r round(mean(nonzero_el_list1),2) `$ & $`r round(mean(nonzero_el_list2),2) `$\\
\hline
Mean MSE LASSO & $`r round(mean(mse_lasso_list1),2) `$ & $`r round(mean(mse_lasso_list2),2) `$\\
\hline
Mean MSE EN  & $`r round(mean(mse_el_list1),2) `$ & $`r round(mean(mse_el_list2),2) `$\\
\hline
\end{tabular}
\end{center}
\end{table}
As we can see from the table, Elastic Net is superior to LASSO in both scenarios terms of MSEs. However, in the first scenario, as far as variable selection is concerned, LASSO performs a better selection, closer to the number of non-zero coefficient of the true $\beta$, when there is a significant correlation structure between predictors. In the second scenario, LASSO selects only one of the two highly correlated parameters, whereas Elastic Net selects almost 3 predictors on average. These results meet our initial expectations.

\subsubsection{Scenario 3}
```{r}
mse_el_list_50 = c()
mse_lasso_list_50 = c()
for (p in 1:1){ # number of predictors varies from 1 to 40
nonzero_lasso_list_40 = c()
nonzero_el_list_40 = c()
mse_el_list = c()
mse_lasso_list = c()
for (sim in 1:1){ # making 50 simulations
library(MASS)

n.train = 20
n.validation = 20
n.test = 200
n = n.train + n.validation + n.test
  
X.train = matrix(rnorm(n.train*p),n.train,p)
X.train = as.matrix(X.train)
X.validation = matrix(rnorm(n.validation*p),n.validation,p)
X.validation = as.matrix(X.validation)
X.test = matrix(rnorm(n.test*p),n.test,p)
X.test = as.matrix(X.test)
X = rbind(X.train,X.validation,X.test)

B = runif(p,-4,4) #randomly set the components of beta
sigma = 3
epsilon = mvrnorm(n=1, Sigma=diag(n), mu=rep(0,n))

Y = X %*% B + sigma * epsilon
Y = as.vector(Y)
Y = scale(Y)

y.train = Y[1 : n.train]
y.validation = Y[(n.train+1) : (n.train+n.validation)]
y.test = Y[(n.train+n.validation+1) : n]

max_iter = 1000
tolerance = 1e-4
lambda_list = seq(0,1,0.01)
mse_list = c()
for (lambda in lambda_list){
  lasso_beta <- lasso_coordinate_descent(X.train, y.train, lambda=lambda,
                                         max_iter, tolerance)
  y_hat = X.validation %*% lasso_beta
  mse_list= c(mse_list, mean((y_hat-y.validation)^2))
}

lambda_min_lasso = lambda_list[which.min(mse_list)]

lasso_beta_min <- lasso_coordinate_descent(X.train, y.train, lambda=lambda_min_lasso, 
                                           max_iter, tolerance)
y_hat_min_lasso = X.test %*% lasso_beta_min
mse_test_lasso = mean((y_hat_min_lasso-y.test)^2)

nonzero_lasso = sum(lasso_beta_min!=0)

mse_list_el = c()
lambda1_list = seq(0,1,0.01)
n1 = length(lambda1_list)
lambda2_list = seq(0,1,0.01)
n2 = length(lambda2_list)
a = 1
matrix = matrix(nrow = n1*n2, ncol = 3)
for (i in 1:n1){
  for (j in 1:n2){
    el_beta <- el_coordinate_descent(X.train, y.train, lambda1=lambda1_list[i],
                                     lambda2=lambda2_list[j], max_iter, tolerance)
    y_hat = X.validation %*% el_beta
    matrix[a,] = c(lambda1_list[i], lambda2_list[j], mean((y_hat-y.validation)^2))
    a = a + 1
   }
}

index = which.min(matrix[,3])
lambda_min_el = matrix[index,]

el_beta_min <- el_coordinate_descent(X.train, y.train, lambda1=lambda_min_el[1],
                                  lambda2=lambda_min_el[2], max_iter, tolerance)
y_hat_min_el = X.test %*% el_beta_min
mse_test_el = mean((y_hat_min_el-y.test)^2)

mse_el_list = c(mse_el_list, mse_test_el)
mse_lasso_list = c(mse_lasso_list, mse_test_lasso)
if (p == 40){
  nonzero_lasso = sum(lasso_beta_min!=0)
  nonzero_el = sum(el_beta_min!=0)
  nonzero_lasso_list_40 = c(nonzero_lasso_list_40, nonzero_lasso )
  nonzero_el_list_40 = c(nonzero_el_list_40, nonzero_el )
}
}
# Storing the value of the mean of the MSE for LASSO and EN for 50 simulations
# and for each value of p
mse_el_list_50 = c(mse_el_list_50, mean(mse_el_list))
mse_lasso_list_50 = c(mse_lasso_list_50, mean(mse_lasso_list))
}
less10_lasso = sum(nonzero_lasso_list_40 < 10)
less10_el = sum(nonzero_el_list_40 < 10)
more20_lasso = sum(nonzero_lasso_list_40 >= 20)
more20_el = sum(nonzero_el_list_40 >= 20)
# Plotting a lines chart to compare trends of MSE in LASSO and EN when p is increasing
plot(1:1,mse_lasso_list_50, type = 'l',ylim = c(0,1.3),col='blue', 
     xlab = 'Predictors',ylab = 'MSE')

lines(1:1,mse_el_list_50, type = 'l',col = 'red')

legend("bottomright",legend = c('Lasso','Elastic Net'), lty = c(1,1), 
       col = c('blue','red'))
```


From the graph we can notice that, as the number of predictors increases, on average, the elastic net always outperforms the LASSO. In addition, the differences between the MSEs computed by the two methods becomes larger.


\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& non-zero$<10$ & $10\le$ non-zero$< 20$ & non-zero$\ge 20$ \\
\hline
Lasso & $`r less10_lasso `$ &  $`r 40- less10_lasso - more20_lasso `$ & $`r more20_lasso `$\\
\hline
EN & $`r less10_el `$ &  $`r 40 - less10_el - more20_el `$ & $`r more20_el `$\\
\hline

\end{tabular}
\end{center}
\end{table}

The number of non-zero coefficients estimated by LASSO and elastic net are reported in the table above. In particular they refer to the case $p=40$ and they are classified in intervals. It can be observed how LASSO never estimates more than 20 ($n$) non-zero coefficients, whereas elastic net selects variable in a way that better reflects the true value of $\beta$. 

\subsection{Limitations and further research}
Our project covers the use of simulated data in three distinct scenarios, offering a comprehensive evaluation of the performances of our algorithms under varying correlation structures and dimensionalities.

However, the project acknowledges a notable weakness related to the encountered issue during the execution of the algorithm, where beta coefficients tended to inflate, potentially leading to convergence problems. While the issue is recognised and addressed, the project does not delve into a comprehensive resolution, leaving room for further investigation into the root causes and potential remedies. Additionally, the reliance on simulated data, while valuable for controlled experimentation, limits the real-world applicability of our project. Integrating real datasets into the analysis could provide insights into the performance of the algorithms in more practical scenarios.

Further analysis could lead to explore additional scenarios or parameter variations to gain a more nuanced understanding of algorithm behaviour. Investigating the impact of varying sample sizes on algorithm performance would contribute insights into scalability. Finally, comparisons with alternative optimisation methods, such as proximal gradient descent or stochastic gradient descent, would enrich the evaluation.

\subsection{Conclusion}

The analysis conducted in this project supports the conclusion that elastic net generally outperforms LASSO, aligning with our initial expectations. However, additional insights emerged from the specific scenarios investigated.
In Scenario 1, it was observed that elastic net exhibited a less favorable variable selection compared to LASSO. Despite this, the MSE consistently favored elastic net, showcasing its robust predictive accuracy in this context.
Scenario 2 revealed a pattern where lasso consistently selected the first and third variables. This aligns with our expectations as there was a high correlation between the first and the second predictor. Instead, elastic net correctly performs as, on average, it is not doing any variable selection.
The most compelling findings emerged in Scenario 3, where elastic net not only outperformed LASSO in all aspects but also demonstrated increasing dominance as the dimensionality ($p$) increased. This underscores the superiority of elastic net in high-dimensional settings.

In summary, the dynamic performance of elastic net, especially in high-dimensional scenarios, suggests its versatility and efficiency, making it a valuable choice for a broad range of applications.  
